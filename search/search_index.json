{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Documentation","text":"<p>This is the unofficial Documentaion for my LLM Projects and Research.</p>"},{"location":"#projects","title":"Projects","text":"<p>All my LLM-Related Projects in a chronological order.</p>"},{"location":"#research","title":"Research","text":"<p>Documentation of the Research I have done in the field of LLMs.</p>"},{"location":"Projects/","title":"Projects","text":""},{"location":"Projects/#example-project","title":"Example Project","text":"<p>Some more example text</p>"},{"location":"Research/","title":"Research","text":""},{"location":"Research/#rag","title":"RAG","text":""},{"location":"Research/#raptor","title":"RAPTOR","text":"<p>RAPTOR: Recursive Abstractive Processing For Tree-Organized Retrieval</p> <p>Source: RAPTOR</p>"},{"location":"Research/#tldr","title":"TL;DR","text":"<p>Raptor implements a so called Raptor Tree. Instead of only chunking data and retrieving from these chunks, Raptor clusters similar Chunks together and generates summaries of those clusters which are then reembedded to create a tree-structure. Upon receiving a query, Raptor traverses the tree, choosing the next branch based on the query and selects relevant summaries and chunks (Leafs of the tree). This allows the LLM to both have high and low level understanding of the context. Problematic is that they rely on a clustering alghorithm and an LLM to create the clusters and the summaries that form the higher level context. If these methods fail, the LLM might not receive relevant chunks, because their roots were never chosen by the tree traversing alghortithm</p>"},{"location":"Research/#implementation-and-usefullnes-for-us","title":"Implementation and Usefullnes for us","text":"<p>In the context of code there could be a possible implementation for a higher level context of TC Projects. The leafs would be single project files like POUs, DUTs, etc. and the roots could be the description of the Project as a whole (Even more abstraction is possible). </p> <p>For an RAG-System of Function-Blocks, this method seems unfeasable, because of it's automatic clustering and summary generation. If the Function-Blocks get clustered manually and higher level constructs are set (e.g Libraries), then this could be an option, althougth this would be a lot of work. </p>"},{"location":"Research/#anytool","title":"AnyTool","text":"<p>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</p> <p>Source: AnyTool</p>"},{"location":"Research/#tldr_1","title":"TL;DR","text":"<p>Implements a hierarchical tree structure similarly to Raptor. But instead of building such a tree, they introduce a novel approach to traverse an existing tree using LLMs. At each level they operate with agents. These agents receive the user query and have a set of functions they are able to call. The higher level agents can get a list of all the elements in a sub-category or the description/s of spefific elements/s. Upon choosing a subcategory, the agent can deploy a new agent that operates on the new lower level. The low-level agents can add APIs to an API Pool. The Agent can then choose to have the API checked. A solver will call the API and try to solve the user's query. In a self-reflection process, another Agent checks wether the query has been solved, or wether the search should continue.</p>"},{"location":"Research/#implementation-and-usefullnes-to-us","title":"Implementation and Usefullnes to us","text":"<p>Looking further than an API-Implementation, this hierarchical structure could be used for code, function blocks and many other implementations.</p> <p>The difficulty with implementing such a construct is the sheer amount of compute needed even for a simple task. There are multiple agents running in parallel only to retrieve feasible options plus an agent to solve and check the results. This makes it nearly impossible to run locally at reasonable sppeds and the cost of running this directly from an inference provider would be quite high.</p>"},{"location":"Research/#frameworks","title":"Frameworks","text":""},{"location":"Research/#dspy-work-in-progress","title":"DSPy (Work in Progress)","text":"<p>Framework for \"Programming\" with LLMs</p> <p>Source: DSPy</p>"},{"location":"Research/#tldr_2","title":"TL;DR","text":"<p>DSPy provides pre-written modules to create tasks like Chain-of-Thought or Question-Answering. One of their big advantages is that one can add depth to a program with a very similar to Layers in PyTorch (It also uses syntax inspired by PyTorch). Instead of having one prompt to do a whole task, a task can be divided into smaller tasks to provide better results, plus certain classes can be reused for similar tasks. The main advantage of DSPy though is what they call their compiler. Once you have finished building your pipeline, it can be compiled with DSPy. Internally, this will build and improve the prompt to achieve better results. Also, it is then possible to create short, generic prompts and not needing to write a specific prompt for every usecase: <pre><code>class BasicQA(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n</code></pre> The compiler will change the prompt to the necessary usecase and will either use the description or the name of the field (here: <code>question</code>) To add steps like Chain-of-Thought, it is as simple as initiating a CoT Module: <pre><code>class GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)\n</code></pre> The <code>dspy.ChainOfThought(GenerateAnswer)</code> will add a Chain-of-Thougth prompt automatically during compilation, signigfically improving the quality</p> <p>To compile the created modules, there are different so called <code>teleprompters</code>. They each improve the prompt in a unique way, adding flexibility to the system. Here is an example of a compilation with a BootStrapFewShot teleprompter, which adds FewShot examples via an LLM: <pre><code>from dspy.teleprompt import BootstrapFewShot\n# Set up a basic teleprompter, which will compile our RAG program.\nteleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n\n# Compile!\ncompiled_rag = teleprompter.compile(RAG(), trainset=trainset)\n</code></pre> Currently, there are 9 Different Teleprompters:</p> <ul> <li><code>LabeledFewShot</code> (For Adding your own few shot examples)</li> <li><code>BootStrapFewShot</code> (For auto-generating few shot examples)</li> <li><code>BootStrapFewShotWithRandomSearch</code> </li> <li><code>BootStrapFewShotWithOptuna</code> (Implements Bayesian Optimization)</li> <li><code>SignatureOptimizer</code></li> <li><code>BayesianSignatureOptimizer</code></li> <li><code>BootstrapFinetune</code></li> <li><code>KNNFewShot</code></li> <li><code>Ensemble</code></li> </ul>"},{"location":"Research/#implementation-and-usefullnes-to-us_1","title":"Implementation and Usefullnes to us","text":"<p>As this is a whole framework, this would dictate the way we would use LLMs in the future. But this framework is very powerful and promising. It is on the other hand quite complex and needs a lot of getting into, especially when using it's wide feature set. Seeing the performance that this framework can deliver even on very complex tasks, I think that it's worth looking into.</p>"},{"location":"Research/#work-in-progress","title":"Work in Progress","text":"<p>The current form of the framework is very powerful when it works and new features get added regularly. But it is definatly unfished. I was not able to recreate most of their \"tutorial\" code and a lot of the provided examples are quite hard to read as they mix a lot of their functionalities with other existing frameworks. </p>"}]}